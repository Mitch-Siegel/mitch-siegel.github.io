<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-03-21T21:46:16-04:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Painstakingly Parsing Pointers</title><link href="http://localhost:4000/blog/2022/03/21/pointers.html" rel="alternate" type="text/html" title="Painstakingly Parsing Pointers" /><published>2022-03-21T18:30:00-04:00</published><updated>2022-03-21T18:30:00-04:00</updated><id>http://localhost:4000/blog/2022/03/21/pointers</id><content type="html" xml:base="http://localhost:4000/blog/2022/03/21/pointers.html"><![CDATA[<h2 id="implementing-pointers">Implementing Pointers</h2>
<p>Since last update, progress has been a bit slow, but I’ve managed to get very basic pointers up and running. Given that there is still only one type supported in the compiler, the basic implementation is fortunately able to mostly ignore (or just make assumptions about) data size. The main steps in implementing pointers were as follows:</p>

<h3 id="pointer-parse-rules">Pointer Parse Rules</h3>
<p>The abilities of pointers necessitate the addition of two main complexities in parsing. The first is the ability to declare variables at different levels of indirection (var vs var * vs var **, etc). This is simply handled by nesting a declared name under a one-legged tree of “dereference” tokens correspondingly deep. The second is the ability to arbitrarily use the dereference operator <code class="language-plaintext highlighter-rouge">*</code> in expressions, to both read from and write to addresses denoted by sub-expressions.</p>

<h3 id="pointers-and-the-symbol-table">Pointers and the Symbol Table</h3>
<p>Because pointer declarations are nested as children of dereference operators, it’s easy to recurse during the first AST traversal to count how many levels of indirection a declaration specifies. Tracking this in the symbol table entry for each variable is then as simple as an integer count of indirection level. This should hold even when implementing different types down the road.</p>

<h3 id="intermediate-code-for-pointers">Intermediate Code for Pointers</h3>
<p>When on the left-hand side of an expression, a single dereference operator signifies a write to the memory address dictated by the pointer subexpression following it (including more dereference tokens, treated the same as dereference operations on the RHS). Similarly, on the right-hand side of an expression, a dereference operator signifies a read from the memory address pointed to by the memory address dictated by its pointer subexpression.</p>

<h3 id="assembly-generation-and-register-allocation">Assembly Generation and Register Allocation</h3>
<p>Since pointer arithmetic ultimately just boils down to regular arithmetic, the only thing to deal with (for now) is just generating instructions to read and write memory when necessary. Optimizing pointer arithmetic is something that can happen down the road, for now it just needs to work. Firing everything up to do basic adaptations of the fibonacci and prime number finders, this works perfectly!</p>

<h2 id="benchmark-improvements">Benchmark Improvements</h2>
<p>Including the source and assembly for the benchmarks in the last blog post cluttered things up too much. For the actual source code and generated assembly, look <a href="https://github.com/Mitch-Siegel/customSystem/tree/main/compiler/benchmarks">here</a>.</p>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>recursion only</th>
      <th>unoptimized pointers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>First 20 Fibonacci numbers:</td>
      <td>859,483</td>
      <td>461</td>
    </tr>
    <tr>
      <td>Just the 20th Fibonacci number:</td>
      <td>328,363</td>
      <td>461</td>
    </tr>
    <tr>
      <td>All primes under 1000:</td>
      <td>6,923,578</td>
      <td>32,108</td>
    </tr>
  </tbody>
</table>

<h2 id="post-benchmark-horrors">Post-Benchmark Horrors</h2>
<p>After running the benchmarks, the logical next step was to implement some other tests. Unfortunately, this is where things fell apart. The overly-simplisic and greedy register allocator I had implemented to get codegen up and running just couldn’t handle everything being thrown at it. It happened to work well enough to hold together for the updated benchmarks, but anything more broke it. Things were ending up in the wrong place, lifetimes were getting mismatched, everything imploded all at once.</p>

<p>This means that optimizing pointers is now a lower priority than implementing proper register allocation and lifetime management. Improving pointer arithmetic, linearization, and implementing indirection level checks will have to come later. The next blog post will more than likely be an overview of the implementation of Linear Scan register allocation to get everything back on track.</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Implementing Pointers Since last update, progress has been a bit slow, but I’ve managed to get very basic pointers up and running. Given that there is still only one type supported in the compiler, the basic implementation is fortunately able to mostly ignore (or just make assumptions about) data size. The main steps in implementing pointers were as follows:]]></summary></entry><entry><title type="html">Compiler Benchmarking and Imminent Features</title><link href="http://localhost:4000/blog/2022/02/23/compiler_benchmarking.html" rel="alternate" type="text/html" title="Compiler Benchmarking and Imminent Features" /><published>2022-02-23T07:30:00-05:00</published><updated>2022-02-23T07:30:00-05:00</updated><id>http://localhost:4000/blog/2022/02/23/compiler_benchmarking</id><content type="html" xml:base="http://localhost:4000/blog/2022/02/23/compiler_benchmarking.html"><![CDATA[<h2 id="compiler-progress">Compiler Progress</h2>
<p>Since the last post, I’ve upgraded every stage of the compilation pipeline to support the things necessary for flow control. At this point, if/else blocks and while loops are up and running almost perfectly. The main issue of keeping track of register states actually didn’t end up being quite as difficult as I expected it to be.</p>

<h2 id="flow-control-and-register-state-tracking">Flow Control and Register State Tracking</h2>
<p>Imagine some function which performs straight-line operations, then evaluates an if/else statement, and then performs some more straight-line operations afterwards. Before the if statement, the state of the registers is easily tracked since we know exactly what operations will happen. But what if the contents of the if block and the else block vary enough that register contents get all switched around? After we exit whichever branch we ended up taking, the registers could be in one of two states. This is no good to continue executing code - how do we know which registers contain what?</p>

<p>My solution to this issue was to implement a sort of “state snapshot” system. Before any branching operation is done, the code generator copies the current register states (register contents, and which registers are live) and pushes the duplicated states to a stack. Then, whether it’s generating code for an if statement or the matching else, it can refer to the most recent state snapshot. This allows the code generator to always start from a known state. Similarly, once code generation for an if or else block is done, there’s a reference for what the machine state should look like on exit from these branches.</p>

<p>Using the snapshot system, the states of the registers can be restored to what they were before a branch was evaluated. Then, when code has been generated for all the branch conditions, the most recent state snapshot is popped off the stack. Now the code generator can move on as if nothing happened. This stack-based approach naturally allows for easy nesting of branching operations too.</p>

<p>This same concept applies to the while loop - whether or not the loop condition is met, the starting state is always known. Because of this, the state can be restored to consistency regardless of the outcome of the branch. Using this system, I should be able to put together a for loop and switch statement without too much extra hassle.</p>

<h2 id="turing-complete-but-not-very-dangerous">Turing Complete, but Not Very Dangerous</h2>
<p>Although the compiler still needs some manual help to assemble, call, and run its output, it’s now at a point where it’s nearing usability to do some interesting stuff. That said, without any way to access memory directly, actually pulling these things off is more trouble than it’s worth. My next major step will be to implement pointers and pointer arithmetic. These will still follow the same typeless, 16 bit unsigned arithmetic I’ve been using so far. From there I hope to move along quickly to arrays, and allow for local array allocation within functions too.</p>

<p>Pointers alone will make the language properly usable, but I think arrays (and the syntactic sugar they provide on top of pointer arithmetic) will really bring things together and add a real edge to it.</p>

<h2 id="compiler-benchmarking-the-bad-the-worse-and-the-recursive">Compiler Benchmarking (The Bad, the Worse, and the Recursive)</h2>
<p>So far, most of my testing has been on silly and meaningless examples just to verify and troubleshoot features. With flow control, I’ve been able to use the Fibonacci sequence and finding primes to test things out. The Fibonacci algorithm I’m using is the simplest possible (and quite poorly performing) recursive algorithm. The prime finder is an atrociously naive implementation which iteratively tries to divide all numbers less than its input to ascertain primality.</p>

<h3 id="fibonacci-numbers">Fibonacci Numbers</h3>
<p>Given the 16 bit word size of the machine, the first 20 numbers in the series seems like a nice round number to benchmark. The 25th entry is just too big at 75,025, so 20 will be the number I’ll use.</p>

<h3 id="primes">Primes</h3>
<p>For the primes benchmark, I decided to find all prime numbers under 1,000, or the first 168 primes. Above 3,000 or so the emulator really starts to chug doing all the iterative division for each number. Maybe 10,000 will be a target for the future, when I can implement a more efficient algorithm.</p>

<h3 id="benchmark-results">Benchmark Results</h3>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>Instructions Executed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>First 20 Fibonacci numbers:</td>
      <td>859,483</td>
    </tr>
    <tr>
      <td>Just the 20th Fibonacci number:</td>
      <td>328,363</td>
    </tr>
    <tr>
      <td>All primes under 1000:</td>
      <td>6,923,578</td>
    </tr>
  </tbody>
</table>

<p>There’s not too much to say about the primes for an implementation this naive, but it’s interesting to see how the exponential recursion really catches up with the Fibonacci series. The 20th entry alone took almost half as many instructions as it took to calculate as the entire series!</p>

<p>It has been really cool to see the source code for these algorithms get compiled and run, but their performance is absolutely woeful. With only functions, local variables, if/else’s, and while loops, what can you really do though? I hope to continually reimplement and benchmark these algorithms as I add features to the language and improve the code generation. As more features come along, I will also be able to add more interesting benchmarks.</p>

<p>Below are the exact source code and assembly outputs for each of the examples. There is some inline assembly used to (hackily) output the calculated values. Even without much analysis, it’s pretty easy to see that the code generation could be better. I’m looking forward to drastically improving these benchmark performances using pointers in the near future!</p>

<h4 id="fibonacci-source-code">Fibonacci Source Code</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># this function computes the nth fibonacci number 
fun fib(var n)
{
    if(n &lt; 2)
        return n;
    else
        return fib(n - 1) + fib(n - 2); 
}

# find all fibonacci numbers up to and including n
fun firstnfibs(var n)
{
    var i = 0;
    while(i &lt; n)
    {
        fib(i + 1);
        asm
        {
            out %r0
        };
        i = i + 1;
    }
}
</code></pre></div></div>

<h4 id="primes-source-code">Primes Source Code</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># software modulo! (are you having fun yet?)
fun modulo(var a, var b)
{
    while(a &gt;= b)
    {
        a = a - b;
    }
    return a;
}

# return whether or not a number is prime
fun isPrime(var n)
{
    var i = 2;
    while(i &lt; n)
    {
        var result = modulo(n, i);
        if(result == 0)
            return 0;

        i = i + 1;
    }
    return 1;
}

fun firstNPrimes(var n)
{
    var i = 2;
    var foundNum = 0;
    while(foundNum &lt; n)
    {
        var result = isPrime(i);
        if(result == 1)
        {
            asm
            {
                out %r2
            };
            foundNum = foundNum + 1;
        }
        i = i + 1;
    }
}
</code></pre></div></div>

<h4 id="fibonacci-compiler-output">Fibonacci Compiler Output</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fib:
	sub %sp, $0
	push %r1
	mov %r0, 4(%bp)
	cmp %r0, $2
	jge fib_1
	mov %r0, 4(%bp)
	jmp fib_done
fib_1:
	mov %r0, 4(%bp)
	sub %r0, $2
	push %r0
	call fib
	mov %r1, 4(%bp)
	sub %r1, $1
	push %r1
	mov %r1, %r0
	call fib
	add %r1, %r0
	mov %r0, %r1
	jmp fib_done
fib_done:
	pop %r1
	add %sp, $0
	ret 1
firstnfibs:
	sub %sp, $2
	push %r1
	push %r2
	mov %r0, $0
firstnfibs_1:
	cmp %r0, 4(%bp)
	jge firstnfibs_2
	mov %r1, %r0
	add %r1, $1
	push %r1
	mov %r2, %r0
	call fib
	out %r0
	add %r2, $1
	mov %r0, %r2
	jmp firstnfibs_1
firstnfibs_2:
firstnfibs_done:
	pop %r2
	pop %r1
	add %sp, $2
	ret 1
</code></pre></div></div>

<h4 id="primes-compiler-output">Primes Compiler Output</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modulo:
	sub %sp, $0
modulo_1:
	mov %r0, 4(%bp)
	cmp %r0, 6(%bp)
	jl modulo_2
	mov %r0, 4(%bp)
	sub %r0, 6(%bp)
	mov 4(%bp), %r0
	jmp modulo_1
modulo_2:
	mov %r0, 4(%bp)
	jmp modulo_done
modulo_done:
	add %sp, $0
	ret 2
isPrime:
	sub %sp, $4
	push %r1
	push %r2
	mov %r0, $2
isPrime_1:
	cmp %r0, 4(%bp)
	jge isPrime_2
	push %r0
	push 4(%bp)
	mov %r1, %r0
	call modulo
	cmp %r0, $0
	jne isPrime_3
	mov %r2, $0
	mov %r0, %r2
	jmp isPrime_done
isPrime_3:
	add %r1, $1
	mov %r0, %r1
	jmp isPrime_1
isPrime_2:
	mov %r0, $1
	jmp isPrime_done
isPrime_done:
	pop %r2
	pop %r1
	add %sp, $4
	ret 1
firstNPrimes:
	sub %sp, $6
	push %r1
	push %r2
	mov %r0, $2
	mov %r1, $0
firstNPrimes_1:
	cmp %r1, 4(%bp)
	jge firstNPrimes_2
	push %r0
	mov %r2, %r0
	call isPrime
	cmp %r0, $1
	jne firstNPrimes_3
	out %r2
	add %r1, $1
firstNPrimes_3:
	add %r2, $1
	mov %r0, -2(%bp)
	mov %r0, %r2
	jmp firstNPrimes_1
firstNPrimes_2:
firstNPrimes_done:
	pop %r2
	pop %r1
	add %sp, $6
	ret 1
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Compiler Progress Since the last post, I’ve upgraded every stage of the compilation pipeline to support the things necessary for flow control. At this point, if/else blocks and while loops are up and running almost perfectly. The main issue of keeping track of register states actually didn’t end up being quite as difficult as I expected it to be.]]></summary></entry><entry><title type="html">Custom System - Beginnings</title><link href="http://localhost:4000/blog/2022/02/07/custom_system.html" rel="alternate" type="text/html" title="Custom System - Beginnings" /><published>2022-02-07T10:45:00-05:00</published><updated>2022-02-07T10:45:00-05:00</updated><id>http://localhost:4000/blog/2022/02/07/custom_system</id><content type="html" xml:base="http://localhost:4000/blog/2022/02/07/custom_system.html"><![CDATA[<h2 id="the-idea">The Idea</h2>
<p>For quite a while now, I’ve had thoughts of trying to design and implement custom ISA’s, programming languages, and even an OS. These ideas never made it much past the idea phase, and I have a number of half-baked project repos lurking on my github private to show for it.</p>

<p>In late 2021, I decided to change my track record with this type of project. I wanted to try to loop all these previous ideas together, so I figured I’d try to design a custom ISA, write an emulator to run it on, and get to work on some sort of compiler for it.</p>

<h2 id="the-architecture">The Architecture</h2>
<p>I started off by designing a simple ISA with just the most basic functionality to get off the ground. The CPU supports an ISA that looks and works a lot like the 80186. For now, everything is 16 bit, with byte-addressable memory. The biggest design feature to me is having a CPU that somewhat mirrors modern systems, with a hardware stack and some general purpose registers. There isn’t really too much more to say for now, other than that it is almost certain to change as I develop the other parts of this project.</p>

<h2 id="the-compiler">The Compiler</h2>
<p>This has been my main area of focus so far. My main goal as of right now is to get this system to a state such that it can run some sort of UNIX-like operating system, and bootstrap its own compiler. Additionally, I want to build something from the ground up that follows a “standard” compilation pipeline.</p>

<h2 id="language-features">Language Features</h2>
<p>My desire to write some sort of operating system means that I am aiming to compile something similar to B or the original implementation of the C language. To get things off the ground, I’m working on a feature set closer to B - everything is typeless, meaning the only data being dealt with is machine words for now.</p>

<p>Currently, I have expression and (very basic) arithmetic parsing up and running. I’m continually working on code generation and register allocation while I implement basic flow control. Once I get past this step, I plan to add arrays and pointers.</p>

<p>It wouldn’t make much sense to have byte addressing capabilities if the machine will only ever run a language that deals in machine words. That’s why I also hope to implement a simple type system to make programming easier. As I explain later in the compiler phases overview, I have already laid a lot of groundwork that will hopefully enable me to transition to supporting different types when the time comes.</p>

<h2 id="compiler-phases">Compiler Phases</h2>

<h3 id="parsing">Parsing</h3>
<p>The parser is a simple recursive descent type parser. It is able to use either one character or one token of lookahead to inform its next action. This has served very well so far. Look for a post with more details about the grammar for the language as the compiler nears completion. The parser builds an abstract syntax tree as it goes, and includes some very basic error detection, hopefully to be improved soon.</p>

<h3 id="symbol-table-generation">Symbol Table Generation</h3>
<p>Once the AST has been generated, symbol tables for the program and all its functions are generated by walking it. These tables contain information about variable/argument counts and names. Right now, it’s pretty overkill for the feature set, but it should make implementing types a whole lot easier when that time comes.</p>

<h3 id="linearization-three-address-code-generation">“Linearization” (three-address code generation)</h3>
<p>The critical thing about the symbol table is that each function entry contains a field for a block of three-address code (TAC). In this step, the AST is walked again, and collapsed down into a series of linear blocks. This step does a lot in terms of getting the program closer to assembly code.</p>

<p>Its main functionality is to collapse all expressions by breaking them down and using temporary variables to hold in-progress pieces of larger expressions. Additionally, it evaluates the structure of if/else blocks. This includes generating the correct labels and jumps for each condition, as well as linearizing the code within the bodies of the statements.</p>

<p>Even though only very basic arithmetic and the ‘if/else’ flow control exist right now, the machinery used in this step should be easily adaptable. This means that as I continue to add features, I should be able to use the existing framework to quickly expand out the feature set of the compiler.</p>

<h3 id="lifetime-evaluation">Lifetime Evaluation</h3>
<p>This step assigns lifetimes to variables in terms of their TAC indices. It uses a simple linear scan method to find when a variable is first and last used. For all points in between, the variable is considered to be “live”. This step also involves some checks on the variables that aren’t temporaries generated during linearization. This enables the compiler to enforce rules about assign-before-declare and use-before-assign errors. This lifetime is absolutely essential for register allocation during code generation.</p>

<h3 id="target-code-generation">Target Code Generation</h3>
<p>A lot of the complexity in figuring out what the output will look like falls to the linearization step. Once a function’s TAC block has been generated, reading that can give a pretty good idea of what the corresponding assembly is going to look like. Code generation bridges the gap between three-address code and assembly.</p>

<p>Since the linearization step involves the use of arbitrary temporary variables, there is a good amount of work in keeping track of what values are where. Additionally, regardless of whether an ‘if’ or its ‘else’ are executed, the registers of the machine need to end up in the same state after the evaluation and execution of that statement. Similar problems will follow for other features as they are implemented.</p>

<p>This is the phase I’m currently working on. It’s taken a lot of rethinks and rewrites, but I think something close to a good solution is taking form as I continue to work on this. Realistically, this phase has always presented the largest and most difficult set of issues. Taking that in stride, things are coming along well, and I’m excited to write an update when I get a fully working version of everything (and then subsequently break it completely with new features).</p>

<h2 id="conclusion">Conclusion</h2>
<p>All in all, things are moving along nicely with the compiler. I’m still not exactly sure what my vision is for the entire project, but that’s fine at this point. As I continue to develop the compiler and add features, I will gradually be able to shift my focus to those things. After I have some reasonably working code generation, adding new features should be much less of an effort, since all the starting pieces will already be in place. Then from there, I can start to think more about the architecture and what exactly I intend to do with it!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[The Idea For quite a while now, I’ve had thoughts of trying to design and implement custom ISA’s, programming languages, and even an OS. These ideas never made it much past the idea phase, and I have a number of half-baked project repos lurking on my github private to show for it.]]></summary></entry><entry><title type="html">Test Post</title><link href="http://localhost:4000/blog/test/1970/01/01/test.html" rel="alternate" type="text/html" title="Test Post" /><published>1970-01-01T00:00:00-05:00</published><updated>1970-01-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/test/1970/01/01/test</id><content type="html" xml:base="http://localhost:4000/blog/test/1970/01/01/test.html"><![CDATA[<h2 id="this-is-a-test">This is a Test</h2>

<p>Thanks for coming, everyone!</p>]]></content><author><name></name></author><category term="blog" /><category term="test" /><summary type="html"><![CDATA[This is a Test]]></summary></entry></feed>