<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-04-18T23:39:48-04:00</updated><id>http://localhost:4000/feed.xml</id><entry><title type="html">Visualizing Progress</title><link href="http://localhost:4000/blog/2022/04/18/visualizing-progress.html" rel="alternate" type="text/html" title="Visualizing Progress" /><published>2022-04-18T18:00:00-04:00</published><updated>2022-04-18T18:00:00-04:00</updated><id>http://localhost:4000/blog/2022/04/18/visualizing-progress</id><content type="html" xml:base="http://localhost:4000/blog/2022/04/18/visualizing-progress.html"><![CDATA[<p>I am currently thinking a lot about what the next steps of this project are, and what that means for the direction of these posts. The compiler needs a good amount more work, but it’s finally starting to become usable. That means that before too long I need to start fleshing out the VM and hardware specs. In the meantime, there’s a lot of cleanup to do, and even more issues with regard to compiler ease-of-use and practicality. With that being said, I now have some concrete visuals and benchmarks for the compiler in general.</p>

<h3 id="visualizing-the-syntax-tree">Visualizing the Syntax Tree</h3>
<p>Over the past week I’ve been playing with graph visualization tools to try and generate an easily-digestible view of the compiler’s syntax tree. I ultimately ended up settling on a utility called <a href="https://graphviz.org/">Graphviz</a>. It is an easy to use package which has incredible power and expressivity to generate all sorts of graphs. The key feature is that it can take input from a file in “DOT Language” containing info about nodes and edges, and then do all the heavy lifting to spit out an image file with a nicely organized graph.</p>

<p>In order to generate the graphs, I added a function that just walks the AST and serializes it node-by-node within the compiler. That output encodes information about each node’s type, text, child, and sibling. Then, I parse that file into a Java program that reconstructs the leftmost child/right sibling tree as it was in the compiler. From there, the Java program refactors the tree so that each node can also know a full list of all of its children. After that, all that’s left to do is generate some formatting parameters for nodes and edges, and simply spit them out in DOT format, Graphviz does the rest! Let’s have a look at the results:</p>

<h3 id="trees-for-simple-expressions">Trees for Simple Expressions</h3>

<p>Here’s what the syntax trees look like for the expressions <code class="language-plaintext highlighter-rouge">var i = 0;</code> and <code class="language-plaintext highlighter-rouge">i = i + 1</code></p>

<p><img src="/declaration-and-arithmetic-for-i.png" alt="declaration-and-arithmetic-for-i" /></p>

<h3 id="trees-for-conditional-logic">Trees for Conditional Logic</h3>
<p>For a simple if/else statement, here’s what things look like:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if(i==2)
    return 1;
else
    return i;
</code></pre></div></div>

<p><img src="/if-else_ast.png" alt="simple-if-else-ast" /></p>

<h3 id="examining-an-entire-function">Examining an Entire Function</h3>

<p>Now that we’ve seen some simple tree representations of code, let’s step it up a notch. Remember our old friend, <code class="language-plaintext highlighter-rouge">firstnfibs()</code>? It took an argument <code class="language-plaintext highlighter-rouge">n</code> and calculated the first <code class="language-plaintext highlighter-rouge">n</code> fibonacci numbers, placing them into an array at a hardcoded location in memory. Then, it looped over that array to print each number out. Here’s its AST graph - can you figure what each statement’s source code would roughly look like?</p>

<p><img src="/firstnfibs_astgraph.png" alt="firstnfibs-ast-graph" /></p>

<p>The first child of the “fun” node is the function name, with the function’s argument type and name parameters as its children. All the other children of the function node are individual statements from the function body. The children are ordered exactly as they are parsed, with the earliest statement leftmost. It’s interesting to see how deep the expression trees go, even for a relatively simple function!</p>

<h3 id="say-whats-the-register-allocator-up-to">Say, What’s the Register Allocator up To?</h3>
<p>Now that the register allocator is working properly, it’s possible to see performance differences by increasing or decreasing the number of registers the allocator is allowed to use. This table lists out the number of instructions needed to the <code class="language-plaintext highlighter-rouge">firstnfibs()</code> function with an argument of 20. Naturally, with fewer registers to work with, more instructions are spent moving values back and forth between the stack and registers.</p>

<table>
  <thead>
    <tr>
      <th>Registers Enabled During Codegen</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Instructions Executed</td>
      <td>1099</td>
      <td>1056</td>
      <td>940</td>
      <td>883</td>
    </tr>
  </tbody>
</table>

<p>Now that my journey out of “just-get-it-working-land” is underway, It’s only a matter of time before I need to expand into some more rigorous performance profiling tools. It’s great to peek inside the compiler and see the AST, and I’d also like to figure out a way to see the basic blocks in the IR as well. I will also need to have access to information about the CPU and what it’s really up to as well, especially as the project moves more into the scope of software. Thanks for reading, and I hope to have a new update with some exciting features soon!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[I am currently thinking a lot about what the next steps of this project are, and what that means for the direction of these posts. The compiler needs a good amount more work, but it’s finally starting to become usable. That means that before too long I need to start fleshing out the VM and hardware specs. In the meantime, there’s a lot of cleanup to do, and even more issues with regard to compiler ease-of-use and practicality. With that being said, I now have some concrete visuals and benchmarks for the compiler in general.]]></summary></entry><entry><title type="html">Compiler Reworks</title><link href="http://localhost:4000/blog/2022/04/12/reworks.html" rel="alternate" type="text/html" title="Compiler Reworks" /><published>2022-04-12T18:00:00-04:00</published><updated>2022-04-12T18:00:00-04:00</updated><id>http://localhost:4000/blog/2022/04/12/reworks</id><content type="html" xml:base="http://localhost:4000/blog/2022/04/12/reworks.html"><![CDATA[<h2 id="cleaning-up-after-last-episode">Cleaning Up After Last Episode</h2>
<p>In the previous update, I detailed how the entire middle and back end of the compiler fell apart. I had managed to get pointers working, but doing so exposed all the weaknesses in my naive first implementations of everything. Since then I have fixed up the IR to a much more capable form, reworked the entire code generator, and implemented proper register allocation.</p>

<h2 id="a-real-ir">A Real IR</h2>
<p>Previously, I had taken the lazy approach of treating the IR as a single linear block of three-address code. This worked fine for the simplistic register allocator I threw together, but with no notion of branching it wasn’t very powerful. The main thing it lacked was the concept of lifetime differences within different branches of code. The new IR is a much more traditional system, where each function is linearized into a sequence of “basic blocks”, each of which contains some code. It differs from a typical basic block implementation in one critical way though - normally a basic block is a section of straight-line code. This would mean that control flows linearly from the start of a block to the end. In my representation, it made more sense to allow branching within blocks, but jumps and calls will only ever target the beginning of a basic block.</p>

<h2 id="linear-scan-register-allocation">Linear Scan Register Allocation</h2>
<p>The need for (and subsequent implementation of) the register allocator went hand-in-hand with the IR improvements. In order to get things back to a working state, the lazy first-come-first-serve allocator with no spilling capabilities had to go. In its place is an implementation of linear scan allocation, with similar state save and restore functionality as before. The allocator itself isn’t anything too special, but it took a lot of machinery under the hood to get it to work properly. The pseudocode in the paper I referenced last article really doesn’t consider much by way of details, so a lot of the development time was spent simply trying and tweaking many iterations of supporting code. Now that the allocator properly supports spilling variables, the stack-based state tracking system I’ve been using for branch control has been beefed up a lot too. Now that variables can actually be spilled, it has a lot more work to do in terms of shuffling values around to make sure state remains consistent between different branch exits.</p>

<h2 id="code-generation">Code Generation</h2>
<p>After reworking the IR and register allocator, putting them to use in the code generator was much easier than before. Overall it’s now a lot easier to see what’s going on and to tweak and build functionality on top of what’s already there. It’s much less of a mess than how I was doing it before, and adding new features should be much more elegant to build up on top of what’s already there.</p>

<h2 id="show-me-some-results">Show me Some Results!</h2>
<p>Admittedly, a lot of what’s discussed in this post is pretty hand-wavy and tough to get a mental model of just from reading. I wanted to throw together some diagrams to include, but it’s already been so long since last post that I decided it’d just be better to get this out there. Now that the whole compilation pipeline is really starting to gel together, I’m going to be able to broaden my focus a bit. I want to cut down on the wordiness of these posts and move towards a more engaging and easily digestible format. There are still a lot of big features on the horizon for the compiler, but I want to branch out into some visualization and profiling tools. The main things I’m looking at right now are some sort of visualiztion for the AST and IR. Keep an eye out for those as a follow-up to the text explanations from this post!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Cleaning Up After Last Episode In the previous update, I detailed how the entire middle and back end of the compiler fell apart. I had managed to get pointers working, but doing so exposed all the weaknesses in my naive first implementations of everything. Since then I have fixed up the IR to a much more capable form, reworked the entire code generator, and implemented proper register allocation.]]></summary></entry><entry><title type="html">Painstakingly Parsing Pointers</title><link href="http://localhost:4000/blog/2022/03/22/pointers.html" rel="alternate" type="text/html" title="Painstakingly Parsing Pointers" /><published>2022-03-22T12:00:00-04:00</published><updated>2022-03-22T12:00:00-04:00</updated><id>http://localhost:4000/blog/2022/03/22/pointers</id><content type="html" xml:base="http://localhost:4000/blog/2022/03/22/pointers.html"><![CDATA[<h2 id="implementing-pointers">Implementing Pointers</h2>
<p>Since the last update, progress has been a bit slow, but I’ve managed to get very basic pointers up and running. The compiler still only supporting one data type makes things easier with regard to data width. The main steps in implementing pointers were as follows:</p>

<h3 id="pointer-parse-rules">Pointer Parse Rules</h3>
<p>The abilities of pointers necessitate the addition of two main complexities in parsing. The first is the ability to declare variables at different levels of indirection (var vs var * vs var **, etc). This is simply handled by nesting a declared name under a one-legged subtree of “dereference” (<code class="language-plaintext highlighter-rouge">*</code>) tokens correspondingly deep. The second is the ability to arbitrarily use the dereference operators in expressions. Identically to c, the dereference operator is used to both read from and write to addresses denoted by sub-expressions depending on its placement.</p>

<h3 id="pointers-and-the-symbol-table">Pointers and the Symbol Table</h3>
<p>Because pointer declarations are nested as children of dereference operators, it’s easy to recurse during the first AST traversal to count how many levels of indirection a declaration specifies. Tracking this in the symbol table entry for each variable is then as simple as an integer count of indirection level. This should hold even when implementing different types down the road. Keeping track of how indirect a variable is will also enable the implementation of indirection level checks in the future. This will make it relatively easy to prevent writing var to var* and other such errors.</p>

<h3 id="intermediate-code-for-pointers">Intermediate Code for Pointers</h3>
<p>When on the left-hand side of an expression, a single dereference operator signifies a write to the memory address dictated by the pointer subexpression following it. A pointer subexpression, including more dereference tokens, is treated the same as a pointer expression on the right-hand side. On the right-hand side of an expression, a dereference operator signifies a read from the memory address pointed to by the memory address dictated by its subexpression.</p>

<h3 id="assembly-generation-and-register-allocation">Assembly Generation and Register Allocation</h3>
<p>Since pointer arithmetic ultimately just boils down to regular arithmetic, the only thing to deal with (for now) is just generating instructions to read and write memory when necessary. Optimizing pointer arithmetic and using different addressing modes are things that can happen down the road. For now it just needs to work.</p>

<h2 id="benchmark-improvements">Benchmark Improvements</h2>
<p>Including the source and assembly for the benchmarks in the last blog post cluttered things up too much. For the actual source code and generated assembly, look <a href="https://github.com/Mitch-Siegel/customSystem/tree/main/compiler/benchmarks">here</a>. Using pointers instead of recursive algorithms to find primes and fibonacci numbers (unsurprisingly) resulted in phenomenal performance gains. In the future I’ll have to think up some more complicated benchmarks to run.</p>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>recursion only</th>
      <th>unoptimized pointers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>First 20 Fibonacci numbers:</td>
      <td>859,483</td>
      <td>461</td>
    </tr>
    <tr>
      <td>Just the 20th Fibonacci number:</td>
      <td>328,363</td>
      <td>461</td>
    </tr>
    <tr>
      <td>All primes under 1000:</td>
      <td>6,923,578</td>
      <td>32,108</td>
    </tr>
  </tbody>
</table>

<h2 id="post-benchmark-horrors">Post-Benchmark Horrors</h2>
<p>After running the benchmarks, the logical next step was to implement some other tests. Unfortunately, this is where things fell apart. The overly-simplisic and greedy register allocator I had implemented to get codegen up and running just couldn’t handle everything being thrown at it. It happened to work well enough to hold together for the updated benchmarks, but anything more broke it. Things were ending up in the wrong place, lifetimes were getting mismatched, everything imploded all at once.</p>

<p>This means that optimizing pointers is now a lower priority than implementing proper register allocation and lifetime management. Improving pointer arithmetic linearization, and adding indirection level checks will have to come later. The next blog post will more than likely be an overview of a proper implementation of Linear Scan register allocation to get everything back on track.</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Implementing Pointers Since the last update, progress has been a bit slow, but I’ve managed to get very basic pointers up and running. The compiler still only supporting one data type makes things easier with regard to data width. The main steps in implementing pointers were as follows:]]></summary></entry><entry><title type="html">Compiler Benchmarking and Imminent Features</title><link href="http://localhost:4000/blog/2022/02/23/compiler_benchmarking.html" rel="alternate" type="text/html" title="Compiler Benchmarking and Imminent Features" /><published>2022-02-23T07:30:00-05:00</published><updated>2022-02-23T07:30:00-05:00</updated><id>http://localhost:4000/blog/2022/02/23/compiler_benchmarking</id><content type="html" xml:base="http://localhost:4000/blog/2022/02/23/compiler_benchmarking.html"><![CDATA[<h2 id="compiler-progress">Compiler Progress</h2>
<p>Since the last post, I’ve upgraded every stage of the compilation pipeline to support the things necessary for flow control. At this point, if/else blocks and while loops are up and running almost perfectly. The main issue of keeping track of register states actually didn’t end up being quite as difficult as I expected it to be.</p>

<h2 id="flow-control-and-register-state-tracking">Flow Control and Register State Tracking</h2>
<p>Imagine some function which performs straight-line operations, then evaluates an if/else statement, and then performs some more straight-line operations afterwards. Before the if statement, the state of the registers is easily tracked since we know exactly what operations will happen. But what if the contents of the if block and the else block vary enough that register contents get all switched around? After we exit whichever branch we ended up taking, the registers could be in one of two states. This is no good to continue executing code - how do we know which registers contain what?</p>

<p>My solution to this issue was to implement a sort of “state snapshot” system. Before any branching operation is done, the code generator copies the current register states (register contents, and which registers are live) and pushes the duplicated states to a stack. Then, whether it’s generating code for an if statement or the matching else, it can refer to the most recent state snapshot. This allows the code generator to always start from a known state. Similarly, once code generation for an if or else block is done, there’s a reference for what the machine state should look like on exit from these branches.</p>

<p>Using the snapshot system, the states of the registers can be restored to what they were before a branch was evaluated. Then, when code has been generated for all the branch conditions, the most recent state snapshot is popped off the stack. Now the code generator can move on as if nothing happened. This stack-based approach naturally allows for easy nesting of branching operations too.</p>

<p>This same concept applies to the while loop - whether or not the loop condition is met, the starting state is always known. Because of this, the state can be restored to consistency regardless of the outcome of the branch. Using this system, I should be able to put together a for loop and switch statement without too much extra hassle.</p>

<h2 id="turing-complete-but-not-very-dangerous">Turing Complete, but Not Very Dangerous</h2>
<p>Although the compiler still needs some manual help to assemble, call, and run its output, it’s now at a point where it’s nearing usability to do some interesting stuff. That said, without any way to access memory directly, actually pulling these things off is more trouble than it’s worth. My next major step will be to implement pointers and pointer arithmetic. These will still follow the same typeless, 16 bit unsigned arithmetic I’ve been using so far. From there I hope to move along quickly to arrays, and allow for local array allocation within functions too.</p>

<p>Pointers alone will make the language properly usable, but I think arrays (and the syntactic sugar they provide on top of pointer arithmetic) will really bring things together and add a real edge to it.</p>

<h2 id="compiler-benchmarking-the-bad-the-worse-and-the-recursive">Compiler Benchmarking (The Bad, the Worse, and the Recursive)</h2>
<p>So far, most of my testing has been on silly and meaningless examples just to verify and troubleshoot features. With flow control, I’ve been able to use the Fibonacci sequence and finding primes to test things out. The Fibonacci algorithm I’m using is the simplest possible (and quite poorly performing) recursive algorithm. The prime finder is an atrociously naive implementation which iteratively tries to divide all numbers less than its input to ascertain primality.</p>

<h3 id="fibonacci-numbers">Fibonacci Numbers</h3>
<p>Given the 16 bit word size of the machine, the first 20 numbers in the series seems like a nice round number to benchmark. The 25th entry is just too big at 75,025, so 20 will be the number I’ll use.</p>

<h3 id="primes">Primes</h3>
<p>For the primes benchmark, I decided to find all prime numbers under 1,000, or the first 168 primes. Above 3,000 or so the emulator really starts to chug doing all the iterative division for each number. Maybe 10,000 will be a target for the future, when I can implement a more efficient algorithm.</p>

<h3 id="benchmark-results">Benchmark Results</h3>

<table>
  <thead>
    <tr>
      <th>Benchmark</th>
      <th>Instructions Executed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>First 20 Fibonacci numbers:</td>
      <td>859,483</td>
    </tr>
    <tr>
      <td>Just the 20th Fibonacci number:</td>
      <td>328,363</td>
    </tr>
    <tr>
      <td>All primes under 1000:</td>
      <td>6,923,578</td>
    </tr>
  </tbody>
</table>

<p>There’s not too much to say about the primes for an implementation this naive, but it’s interesting to see how the exponential recursion really catches up with the Fibonacci series. The 20th entry alone took almost half as many instructions as it took to calculate as the entire series!</p>

<p>It has been really cool to see the source code for these algorithms get compiled and run, but their performance is absolutely woeful. With only functions, local variables, if/else’s, and while loops, what can you really do though? I hope to continually reimplement and benchmark these algorithms as I add features to the language and improve the code generation. As more features come along, I will also be able to add more interesting benchmarks.</p>

<p>Below are the exact source code and assembly outputs for each of the examples. There is some inline assembly used to (hackily) output the calculated values. Even without much analysis, it’s pretty easy to see that the code generation could be better. I’m looking forward to drastically improving these benchmark performances using pointers in the near future!</p>

<h4 id="fibonacci-source-code">Fibonacci Source Code</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># this function computes the nth fibonacci number 
fun fib(var n)
{
    if(n &lt; 2)
        return n;
    else
        return fib(n - 1) + fib(n - 2); 
}

# find all fibonacci numbers up to and including n
fun firstnfibs(var n)
{
    var i = 0;
    while(i &lt; n)
    {
        fib(i + 1);
        asm
        {
            out %r0
        };
        i = i + 1;
    }
}
</code></pre></div></div>

<h4 id="primes-source-code">Primes Source Code</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># software modulo! (are you having fun yet?)
fun modulo(var a, var b)
{
    while(a &gt;= b)
    {
        a = a - b;
    }
    return a;
}

# return whether or not a number is prime
fun isPrime(var n)
{
    var i = 2;
    while(i &lt; n)
    {
        var result = modulo(n, i);
        if(result == 0)
            return 0;

        i = i + 1;
    }
    return 1;
}

fun firstNPrimes(var n)
{
    var i = 2;
    var foundNum = 0;
    while(foundNum &lt; n)
    {
        var result = isPrime(i);
        if(result == 1)
        {
            asm
            {
                out %r2
            };
            foundNum = foundNum + 1;
        }
        i = i + 1;
    }
}
</code></pre></div></div>

<h4 id="fibonacci-compiler-output">Fibonacci Compiler Output</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fib:
	sub %sp, $0
	push %r1
	mov %r0, 4(%bp)
	cmp %r0, $2
	jge fib_1
	mov %r0, 4(%bp)
	jmp fib_done
fib_1:
	mov %r0, 4(%bp)
	sub %r0, $2
	push %r0
	call fib
	mov %r1, 4(%bp)
	sub %r1, $1
	push %r1
	mov %r1, %r0
	call fib
	add %r1, %r0
	mov %r0, %r1
	jmp fib_done
fib_done:
	pop %r1
	add %sp, $0
	ret 1
firstnfibs:
	sub %sp, $2
	push %r1
	push %r2
	mov %r0, $0
firstnfibs_1:
	cmp %r0, 4(%bp)
	jge firstnfibs_2
	mov %r1, %r0
	add %r1, $1
	push %r1
	mov %r2, %r0
	call fib
	out %r0
	add %r2, $1
	mov %r0, %r2
	jmp firstnfibs_1
firstnfibs_2:
firstnfibs_done:
	pop %r2
	pop %r1
	add %sp, $2
	ret 1
</code></pre></div></div>

<h4 id="primes-compiler-output">Primes Compiler Output</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>modulo:
	sub %sp, $0
modulo_1:
	mov %r0, 4(%bp)
	cmp %r0, 6(%bp)
	jl modulo_2
	mov %r0, 4(%bp)
	sub %r0, 6(%bp)
	mov 4(%bp), %r0
	jmp modulo_1
modulo_2:
	mov %r0, 4(%bp)
	jmp modulo_done
modulo_done:
	add %sp, $0
	ret 2
isPrime:
	sub %sp, $4
	push %r1
	push %r2
	mov %r0, $2
isPrime_1:
	cmp %r0, 4(%bp)
	jge isPrime_2
	push %r0
	push 4(%bp)
	mov %r1, %r0
	call modulo
	cmp %r0, $0
	jne isPrime_3
	mov %r2, $0
	mov %r0, %r2
	jmp isPrime_done
isPrime_3:
	add %r1, $1
	mov %r0, %r1
	jmp isPrime_1
isPrime_2:
	mov %r0, $1
	jmp isPrime_done
isPrime_done:
	pop %r2
	pop %r1
	add %sp, $4
	ret 1
firstNPrimes:
	sub %sp, $6
	push %r1
	push %r2
	mov %r0, $2
	mov %r1, $0
firstNPrimes_1:
	cmp %r1, 4(%bp)
	jge firstNPrimes_2
	push %r0
	mov %r2, %r0
	call isPrime
	cmp %r0, $1
	jne firstNPrimes_3
	out %r2
	add %r1, $1
firstNPrimes_3:
	add %r2, $1
	mov %r0, -2(%bp)
	mov %r0, %r2
	jmp firstNPrimes_1
firstNPrimes_2:
firstNPrimes_done:
	pop %r2
	pop %r1
	add %sp, $6
	ret 1
</code></pre></div></div>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Compiler Progress Since the last post, I’ve upgraded every stage of the compilation pipeline to support the things necessary for flow control. At this point, if/else blocks and while loops are up and running almost perfectly. The main issue of keeping track of register states actually didn’t end up being quite as difficult as I expected it to be.]]></summary></entry><entry><title type="html">Custom System - Beginnings</title><link href="http://localhost:4000/blog/2022/02/07/custom_system.html" rel="alternate" type="text/html" title="Custom System - Beginnings" /><published>2022-02-07T10:45:00-05:00</published><updated>2022-02-07T10:45:00-05:00</updated><id>http://localhost:4000/blog/2022/02/07/custom_system</id><content type="html" xml:base="http://localhost:4000/blog/2022/02/07/custom_system.html"><![CDATA[<h2 id="the-idea">The Idea</h2>
<p>For quite a while now, I’ve had thoughts of trying to design and implement custom ISA’s, programming languages, and even an OS. These ideas never made it much past the idea phase, and I have a number of half-baked project repos lurking on my github private to show for it.</p>

<p>In late 2021, I decided to change my track record with this type of project. I wanted to try to loop all these previous ideas together, so I figured I’d try to design a custom ISA, write an emulator to run it on, and get to work on some sort of compiler for it.</p>

<h2 id="the-architecture">The Architecture</h2>
<p>I started off by designing a simple ISA with just the most basic functionality to get off the ground. The CPU supports an ISA that looks and works a lot like the 80186. For now, everything is 16 bit, with byte-addressable memory. The biggest design feature to me is having a CPU that somewhat mirrors modern systems, with a hardware stack and some general purpose registers. There isn’t really too much more to say for now, other than that it is almost certain to change as I develop the other parts of this project.</p>

<h2 id="the-compiler">The Compiler</h2>
<p>This has been my main area of focus so far. My main goal as of right now is to get this system to a state such that it can run some sort of UNIX-like operating system, and bootstrap its own compiler. Additionally, I want to build something from the ground up that follows a “standard” compilation pipeline.</p>

<h2 id="language-features">Language Features</h2>
<p>My desire to write some sort of operating system means that I am aiming to compile something similar to B or the original implementation of the C language. To get things off the ground, I’m working on a feature set closer to B - everything is typeless, meaning the only data being dealt with is machine words for now.</p>

<p>Currently, I have expression and (very basic) arithmetic parsing up and running. I’m continually working on code generation and register allocation while I implement basic flow control. Once I get past this step, I plan to add arrays and pointers.</p>

<p>It wouldn’t make much sense to have byte addressing capabilities if the machine will only ever run a language that deals in machine words. That’s why I also hope to implement a simple type system to make programming easier. As I explain later in the compiler phases overview, I have already laid a lot of groundwork that will hopefully enable me to transition to supporting different types when the time comes.</p>

<h2 id="compiler-phases">Compiler Phases</h2>

<h3 id="parsing">Parsing</h3>
<p>The parser is a simple recursive descent type parser. It is able to use either one character or one token of lookahead to inform its next action. This has served very well so far. Look for a post with more details about the grammar for the language as the compiler nears completion. The parser builds an abstract syntax tree as it goes, and includes some very basic error detection, hopefully to be improved soon.</p>

<h3 id="symbol-table-generation">Symbol Table Generation</h3>
<p>Once the AST has been generated, symbol tables for the program and all its functions are generated by walking it. These tables contain information about variable/argument counts and names. Right now, it’s pretty overkill for the feature set, but it should make implementing types a whole lot easier when that time comes.</p>

<h3 id="linearization-three-address-code-generation">“Linearization” (three-address code generation)</h3>
<p>The critical thing about the symbol table is that each function entry contains a field for a block of three-address code (TAC). In this step, the AST is walked again, and collapsed down into a series of linear blocks. This step does a lot in terms of getting the program closer to assembly code.</p>

<p>Its main functionality is to collapse all expressions by breaking them down and using temporary variables to hold in-progress pieces of larger expressions. Additionally, it evaluates the structure of if/else blocks. This includes generating the correct labels and jumps for each condition, as well as linearizing the code within the bodies of the statements.</p>

<p>Even though only very basic arithmetic and the ‘if/else’ flow control exist right now, the machinery used in this step should be easily adaptable. This means that as I continue to add features, I should be able to use the existing framework to quickly expand out the feature set of the compiler.</p>

<h3 id="lifetime-evaluation">Lifetime Evaluation</h3>
<p>This step assigns lifetimes to variables in terms of their TAC indices. It uses a simple linear scan method to find when a variable is first and last used. For all points in between, the variable is considered to be “live”. This step also involves some checks on the variables that aren’t temporaries generated during linearization. This enables the compiler to enforce rules about assign-before-declare and use-before-assign errors. This lifetime is absolutely essential for register allocation during code generation.</p>

<h3 id="target-code-generation">Target Code Generation</h3>
<p>A lot of the complexity in figuring out what the output will look like falls to the linearization step. Once a function’s TAC block has been generated, reading that can give a pretty good idea of what the corresponding assembly is going to look like. Code generation bridges the gap between three-address code and assembly.</p>

<p>Since the linearization step involves the use of arbitrary temporary variables, there is a good amount of work in keeping track of what values are where. Additionally, regardless of whether an ‘if’ or its ‘else’ are executed, the registers of the machine need to end up in the same state after the evaluation and execution of that statement. Similar problems will follow for other features as they are implemented.</p>

<p>This is the phase I’m currently working on. It’s taken a lot of rethinks and rewrites, but I think something close to a good solution is taking form as I continue to work on this. Realistically, this phase has always presented the largest and most difficult set of issues. Taking that in stride, things are coming along well, and I’m excited to write an update when I get a fully working version of everything (and then subsequently break it completely with new features).</p>

<h2 id="conclusion">Conclusion</h2>
<p>All in all, things are moving along nicely with the compiler. I’m still not exactly sure what my vision is for the entire project, but that’s fine at this point. As I continue to develop the compiler and add features, I will gradually be able to shift my focus to those things. After I have some reasonably working code generation, adding new features should be much less of an effort, since all the starting pieces will already be in place. Then from there, I can start to think more about the architecture and what exactly I intend to do with it!</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[The Idea For quite a while now, I’ve had thoughts of trying to design and implement custom ISA’s, programming languages, and even an OS. These ideas never made it much past the idea phase, and I have a number of half-baked project repos lurking on my github private to show for it.]]></summary></entry><entry><title type="html">Test Post</title><link href="http://localhost:4000/blog/test/1970/01/01/test.html" rel="alternate" type="text/html" title="Test Post" /><published>1970-01-01T00:00:00-05:00</published><updated>1970-01-01T00:00:00-05:00</updated><id>http://localhost:4000/blog/test/1970/01/01/test</id><content type="html" xml:base="http://localhost:4000/blog/test/1970/01/01/test.html"><![CDATA[<h2 id="this-is-a-test">This is a Test</h2>

<p>Thanks for coming, everyone!</p>]]></content><author><name></name></author><category term="blog" /><category term="test" /><summary type="html"><![CDATA[This is a Test]]></summary></entry></feed>